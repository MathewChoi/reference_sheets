\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{geometry}
\geometry{a4paper, landscape, margin=0.25in}

\begin{document}
\begin{multicols}{2}
\section{Properties of Expectation and Variance}


\begin{tabular} {|l|l|}
\hline
Expectation & Variance \\ \hline
$E[\bar{Y}] =  \mu $ & $V(\bar{Y}) = \sigma^2/n $ \\ \hline
$E[\Sigma Y] = n\mu $ & $V(\Sigma Y) = n*\sigma^2 $ \\ \hline
\end{tabular}

\section{Sampling Distributions}
\begin{tabular}{| c | c | c | c |}
\hline
Name & Density Function & $\mu$  & $\sigma^2$ \\ \hline
Normal & $f(y) = \dfrac{1}{\sqrt{2\pi\sigma^2}}*e^{-(y - \mu)^2 / 2\sigma^2}$ & $\mu$ & $\sigma^2$ \\ \hline
Uniform & $f(y) = \dfrac{1}{b-a}$ & $\dfrac{a+b}{2}$ & $\dfrac{(b-a)^2}{12}$ \\ \hline
Exponential & $f(y) = \dfrac{1}{\beta} e^{-y/\beta}$ & $\beta$ & $\beta^2$ \\ \hline
Chi-Squared & $f(y) = \dfrac{y^{\nu/2-1}e^{-y/2}}{2^{\nu/2}\Gamma(\nu/2)}$ & $\nu$ & $2\nu$ \\ \hline
Binomial & $p(y) = \binom{n}{y} p^y (1-p)^n-y$ & $np$ & $np(1-p)$ \\ \hline
Poisson & $p(y) = \dfrac{e^{-\lambda}\lambda^y}{y!}$ & $\lambda$ & $\lambda$ \\ \hline
Geometric & $p(y) = p(1-p)^{y-1}$ & $\dfrac{1}{p}$ & $\dfrac{1-p}{p^2}$ \\ \hline
Bernoulli & $f(k;y) = p^k(1-p)^{1-k}$ & $p$ & $p(1-p)$ \\ \hline
T-distribution & $f(y) = \dfrac{\Gamma((\nu+1)/2)}{\sqrt{\pi\nu}\Gamma(\nu/2)} (1+\dfrac{t^2}{\nu})^{-(\nu+1)/2}$ & $0, \nu>1$ & $\dfrac{\nu}{\nu-2}, \nu>2$ \\ \hline
F-distribution & $f(w) = \dfrac{\Gamma((m+n)/2)m^{m/2}n^{n/2}w^{m/2-1}}{\Gamma(m/2)\Gamma(n/2)(n+m*w)^{(m+n)/2}}$ & $$ & $\sigma^2$ \\ \hline
\end{tabular}\\
\textbf{F-distribution:} $Let U = \chi^2(n) and V = \chi^2(m) then  F = \dfrac{V/m}{U/n}$, denoted F(m,n)\\
\textbf{T-distribution:} $Let Z = N(0,1) and W = \chi^2(\nu). If Z and W are independent, T = \dfrac{Z}{\sqrt{W/\nu}} has the T-distribution w/ \nu dof$\\ 
\textbf{Thm:} Z = N(0,1) implies $\Sigma Z^2 = \chi^2(n)$ \\ 
\textbf{Thm:} $\dfrac{(n-1)s^2}{\sigma^2} = \dfrac{\Sigma (Y-\bar{Y})^2}{\sigma^2} = \chi^2(n-1)$

\section{Estimation}
\textbf{Bias:} $B(\hat{Theta}) = E[\hat{\Theta}] - \theta$ \\
\textbf{Mean Square Error:} 
$MSE(\hat{\Theta}) = E[(\hat{\Theta} - \Theta)] = V(\hat{\Theta}) + B(\hat{\Theta})^2$ \\
\textbf{Error of Estimation:} $\epsilon = |\hat{\Theta} - \Theta|$\\
\textbf{Efficiency:} $eff(\hat{\Theta_1}, \hat{\Theta_2}) = \dfrac{V(\hat{\Theta_2})}{V(\hat{\Theta_1})}$
\textbf{Consistency:} $lim_{n-inf}P(|\hat{\Theta_n}-\Theta|<= \epsilon) = 1; lim_{n-inf}P(|\hat{\Theta_n}-\Theta|> \epsilon) = 0$ \\
\textbf{Thm:} An unbiased estimator $\hat{\Theta_n} is a consistent estimator for \Theta if lim_{n-\inf}V(\hat{\Theta}) = 0$ \\
\textbf{Sufficiency:} Statistic $U = g(Y_{1:n}) is sufficient for \Theta if the conditional distribution of Y_{1:n}, given U doesn't depend on \Theta$
\textbf{Thm:} U is sufficient for $\Theta IFF the L(y_{1:n}|\Theta) = g(U, \Theta)*h(y_{1:n})$\\
\textbf{Min Var Unbiased Estimator:} a sufficient statistic that has been transformed into an unbiased estimator\\
\textbf{Method of Moments:} use the 1st and 2nd moment (if there are 2 param) and use them to find values for the parameters\\
\textbf{Method of Maximum Likelihood:} Take the (partial) derivative of the likelihood function and find the parameters that will maximize its value. Apply ln() on the likelihood if needed.

\section{Order Statistics}
\begin{tabular}{|l|l|}
\hline
\textbf{$Y_{(1)}$} & \textbf{$Y_{(n)}$} \\ \hline
$F_{Y_{(1)}}(y) = 1-[1-F_Y(y)]^{n}$ & $F_{Y_{(n)}}(y) = [F_Y(y)]^{n}$\\ \hline
$f_{Y_{(1)}}(y) = n[1-F_Y(y)]^{n-1}*f_Y(y) $ &
$f_{Y_{(n)}}(y) = n[F_Y(y)]^{n-1}*f_Y(y) $\\ \hline
\end{tabular}

\section{Confidence Intervals}
\begin{tabular}{| l | l | l |}
\hline
Name & Assumptions & Confidence Interval \\ \hline
Mean & $\sigma known$ & $\bar{Y} \pm Z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}}$ \\ \hline

& $\sigma unknown$ & $\bar{Y} \pm t_{\alpha/2}(n-1)\dfrac{s}{\sqrt{n}}$ \\ \hline

Diff of Means & $n_i>=30, \sigma_i known$ & $\bar{Y_1} - \bar{Y_2} \pm Z_{\alpha/2}\sqrt{\dfrac{\sigma_1^2}{n_1}+\dfrac{\sigma_2^2}{n_2}}$ \\ \hline

& $\sigma_i unknown, not equal$ & $\bar{Y_1} - \bar{Y_2} \pm t_{\alpha/2}(dof_s)\sqrt{\dfrac{s_1^2}{n_1}+\dfrac{s_2^2}{n_2}}$\\ \hline

& $\sigma_i unknown, equal$ & $\bar{Y_1} - \bar{Y_2} \pm t_{\alpha/2}(dof_p)S_p\sqrt{\dfrac{1}{n_1}+\dfrac{1}{n_2}}$\\ \hline

Prop & $n>=9\dfrac{max(p,q)}{min(p,q)}$ & $\hat{P} \pm Z_{\alpha/2}\sqrt{\dfrac{\hat{p}\hat{q}}{n}}$ \\ \hline

Diff of Prop & $n_i>=9\dfrac{max(p_i,q_i)}{min(p_i,q_i)}$ & $\hat{P_1} - \hat{P_2} \pm Z_{\alpha/2}\sqrt{\dfrac{\hat{p_1}\hat{q_1}}{n_1}+\dfrac{\hat{p_2}\hat{q_2}}{n_2}}$ \\ \hline

Var & $pop norm distr$ & $(\dfrac{(n-1)s^2}{\chi_{\alpha/2}^2(n-1)}, \dfrac{(n-1)s^2}{\chi_{1-\alpha/2}^2(n-1)})$ \\ \hline

Ratio of Var & $pop norm distr$ & $(\dfrac{s_1^2}{s_2^2F_{\alpha/2}(n_1-1, n_2-1)}, \dfrac{s_1^2F_{\alpha/2}(n_2-1, n_1-1)}{s_2^2})$ \\ \hline
\end{tabular}
\textbf{$dof_s$} = $\dfrac{(A + B)^2}{\dfrac{A^2}{n_1-1} + \dfrac{B^2}{n_2-1}}, A = s_1^2/n_1, B = s_2^2/n_2$ \\
\textbf{$dof_p$} = $n_1 + n_2 - 2$ \\
\textbf{$S_p^2$} = $\dfrac{(n_1-1)s_1^2+(n_2-1)s_2^2}{(n_1+n_2-2)}$\\

\section{Test Statistics}
TODO
\end{multicols}
\end{document}
